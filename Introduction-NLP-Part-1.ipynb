{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profanity-findings in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since I have a huge interest in comedy, I decided to harvest some data in that area and with different tools (presented below and further in this notebook collection) I will share some insights of how to see results in various ways using NLP (Natrual Language Processing)\n",
    "# This is part 1 out of 2:\n",
    "## Set-up<br>Harvest data<br>Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - - - - - - Step 1: Set-up - - - - - -\n",
    "### Before gathering, cleaning or other type of processing of data, it is vital to have a good set-up.<br> Decide what to strive to achieve and a purpose. For this case, I wonder how much three comedians swear.\n",
    "## Goal: Who swears the most between George Carlin, Ricky Gervais and Jim Jefferies?\n",
    "### For this, we will:<br>- Measure size of content for each comedian<br>- See the top 10 most common words for each comedian<br>- See the top 10 least common words for each comedian<br>- See which comedian that used the most profanity words<br>- Discover the amount of times a swearword has been said for each comedian<br><br> Transcripts from comedians: Ricky Gervais, Jim Jefferies and George Carlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "#### I have noticed that there is (basically) two ways of choosing when to import libraries, either at the very top of each notebook or along the way.<br> I will import the libraries at the point I need them, I think it's clearer to see when they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries needed for this notebook: BeautifulSoup, pickle, pandas, request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - - - - - - Step 2: Harvest data - - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will harvest transcripts from a website called 'scrapsfromtheloft.com'.<br> We will then find a suitable way to present the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for harvesting data from URLs\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function gets urls from below and search for p-tags\n",
    "def get_url_convert_to_transcript(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find_all('p')]\n",
    "    print(url)\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "urls = ['https://scrapsfromtheloft.com/2020/07/08/jim-jefferies-intolerant-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2019/09/12/george-carlin-dumb-americans-transcript/',\n",
    "        'https://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comedian last name, usefull as keys later\n",
    "\n",
    "comedians_last_name = ['jefferies', 'carlin', 'gervais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function and download data\n",
    "transcripts = [get_url_convert_to_transcript(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pickle to save and upload data with ease\n",
    "import pickle as pkl\n",
    "\n",
    "# Creates a new folder and stores the transcripts there\n",
    "!mkdir transcripts\n",
    "\n",
    "# This funktion will set each comedian as a key to their transcripts\n",
    "for every, comedian in enumerate(comedians_last_name):\n",
    "    with open('transcripts/' + comedian + \".txt\", \"wb\") as file:\n",
    "        pkl.dump(transcripts[every], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data as dict\n",
    "\n",
    "data = {}\n",
    "\n",
    "for every, comedian in enumerate(comedians_last_name):\n",
    "    with open(\"transcripts/\" + comedian + \".txt\", \"rb\") as file:\n",
    "        data[comedian] = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call keys, if correct - three lastnames shall appear\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 10 first objects (sentences) from 'jefferies' to make\n",
    "# sure that the data has been loaded and that the key is correct.\n",
    "\n",
    "data['jefferies'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not look at Carlin as well?\n",
    "\n",
    "data['carlin'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And Gervais just for the fun of it? :-)\n",
    "\n",
    "data['gervais'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the function below, we can\n",
    "# ... print out the first (next) key\n",
    "\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, the data has the comedian-name as key but uses<br> the list-format for the corpus. Let's convert that into string-format <br>for easier use in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_list_to_string(text_list):\n",
    "    converted_format = ' '.join(text_list)\n",
    "    return converted_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls the function above and sets keys and values, keys\n",
    "# ... being comedian names. This ends up in a dict-format.\n",
    "\n",
    "data_string = {key: [reshape_list_to_string(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using data as Python's own dict-format is fine. However, I personally<br> prefer using a \"Data frame\" from the Pandas-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pandas default in width is quite narrow, so I like to\n",
    "# expand that as much as possible\n",
    "pd.set_option('max_colwidth', 999999999)\n",
    "\n",
    "# The transpose-function swiches place between axis for easier view\n",
    "# ... setting comedian name as Y-axis and transcripts as X-axis\n",
    "df = pd.DataFrame.from_dict(data_string).transpose()\n",
    "df.columns = ['transcript']\n",
    "\n",
    "# Sorts objects by axis, alphabetically\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .loc-feature allow us to retrieve rows by calling\n",
    "# a key from the data frame\n",
    "df.transcript.loc['gervais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the data type, it should say:\n",
    "#    transcript   object\n",
    "#    dtype:       object\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "# Pandas way of handling data is by making them dtype: object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape presents the number of columns of each axis:\n",
    "# ... First vertical (|) and then horizontal (-)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data looks okay and is easy to select the key(s) we want. It's time to enter the next step.<br>\n",
    "## - - - - - - Step 3: Cleaning - - - - - -\n",
    "### Cleaning data in a nutshell means to get rid of unnessecary information and remove symbols, charaters and/or numbers that we don't want to (or can't) use. It also gives us the opportuneity to divide data into chunks if we would want to.\n",
    "#### By using regex, a powerful library for going through big mass of data and apply specific actions, lots of cleaning can be made quite simple. <br>First of, we will do somethings that's good practice to do no matter what text-data that's being pre-processed and that is:<br> * Make all text into lower case.<br> * Remove symbols as well as numbers that are not useful.<br>Other than that, I explain within the code what's happening.\n",
    "### Note: re.sub parameter explained --> (replace what, with this, here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When looking through the texts, I notice that some words within brackets ( [ ] ) are not words said by the actual comedian but rather printed noices such as 'laughter' and 'applause'. These are words I don't want to use in the process later on, so I am going to remove them.<br> I also notice a HTML syntax, \\n , which indicates a new line. Let's remove that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use re (RegEx) and string to clean this data\n",
    "\n",
    "import re\n",
    "\n",
    "def cleaning_session(corpus):\n",
    "    \n",
    "    # Converting all characters to lower case\n",
    "    corpus = corpus.lower()\n",
    "    \n",
    "    # Removes anything within brackets (reason: No valid text-data there)\n",
    "    corpus = re.sub('\\[.*?\\]', '', corpus)\n",
    "    \n",
    "    # Replaces visual breaks for new line (\\n) with a space\n",
    "    corpus = re.sub('\\n', ' ', corpus)\n",
    "    \n",
    "    # Replace any symbols that are NOT characters with a space but keep apostrophies\n",
    "    corpus = re.sub(r\"[^a-z\\’]\", \" \", corpus)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# Note: Running this cell only configures the function, we'll run it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the data we have so far in its variable and create a copy.\n",
    "# ... Good practice when experimenting\n",
    "\n",
    "df_copy = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda is a quicker way of witing code\n",
    "# Here we simply put the function in a variable\n",
    "\n",
    "cleaning = lambda x: cleaning_session(x)\n",
    "\n",
    "# Above function is doing the exact same as the following function:\n",
    "# def cleaning(x):\n",
    "    # return cleaning_session(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activates cleaning session\n",
    "\n",
    "clean_data = pd.DataFrame(df_copy.transcript.apply(cleaning))\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To always know what to replace/remove can be a bit tricky, so I saved some other useful regex-functions with description here for you to browse.\n",
    "#### Last updated 2020 with python 3.8.3<br>\n",
    "\n",
    "<b>Remove quotation marks and tripple punctations</b><br>corpus = re.sub('[‘’“”…]', '', corpus)<br><br><b>Remove dash-symbols (-)</b><br>corpus = re.sub('[–]', '', corpus)<br><br><b>Remove visual breaks for new line (\\n), add space instead</b><br>corpus = re.sub('\\n', ' ', corpus)<br><br><b>Removes all punctuations</b><br>corpus = re.sub('[%s]' % re.escape(string.punctuation), '', corpus)<br><br><b>Remove digits before and after alphanumeric characters<br>\\w\\d\\w =  (a-z, 0-9).<br>Asterix * = 0+ times.<br>... If a word is surrounded by letters or digits, remove that word.\n",
    "</b><br>corpus = re.sub('\\w*\\d\\w*', '', corpus)<br><br><b>Removes anything within curly brackets</b><br>corpus = re.sub('\\{.*?\\}', '', corpus)<br><br><b>Remove white-spaces</b><br>without_spaces = [re.sub(r'\\s+', '', item) for item in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This whole process might look messy to a human eye, we need to remember that all this is about preparing the data for a computer to process.\n",
    "\n",
    "### Let's save the file! The saved file will end up in the same folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_pickle(\"clean_corpus.pkl\")\n",
    "\n",
    "print(\"Pickled complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------- Next ----------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of part 1/2, as a sum we have:<br>- Set goals<br>- Harvested HTML data<br>- Cleaned text data<br><br>In the following notebook, we will remove so called stop-words, create a DTM and plot words.<br>This is commonly called EDA - Exploratory Data Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thank you for visiting my NLP notebook, I hope you liked it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
