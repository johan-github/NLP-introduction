{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the part 2/3 of my series in processig text in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you haven't viewed my previous part, I highly recommend it. That way it's easier to follow in this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this part, we will:<br>- Tokenize words<br>- Remove stop words<br>- Create a DTM<br>- Print top 10 most said words for each comedian<br>- Print 10 least said words for each comedian<br>- Measure amount of profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries needed for this notebook: pandas, sklearn, matplotlib and wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import pickled data using pandas\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 999999999)\n",
    "\n",
    "transcripts = pd.read_pickle(\"clean_corpus.pkl\")\n",
    "\n",
    "transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM - Document Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With a DTM we can present each word from each comedian and by using CountVectorizer<br>we can tokenize (isolate) each word and count how many times a specific word has been said.<br><br>By inserting ---> stop_words='english' <--- inside the CountVectorizer parameter, we remove<br> unnecessary words like \"are\", \"we\", \"is\", \"the\" et.c.<br><br>This section covers: Tokenization, stop-words, DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "cv_data = cv.fit_transform(transcripts.transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_data = pd.DataFrame(cv_data.toarray(), columns = cv.get_feature_names())\n",
    "dtm_data.index = transcripts.index\n",
    "dtm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As seen from this data, stop-words remove common words but not necessarily strange words or 'non-words' such as \"aaagh\". This can manually be taken care of by adding your own stop-words to be used istead or on top of basic-stopwords. For this case, it is not really vital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's flip the DTM-axis for easier view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_dtm_data = dtm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_dtm_data = copy_dtm_data.transpose()\n",
    "copy_dtm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will print out the top 10 most said words said by each comedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = {}\n",
    "\n",
    "for comedian in copy_dtm_data.columns:\n",
    "    top = copy_dtm_data[comedian].sort_values(ascending=False).head(10)\n",
    "    most_common_words[comedian] = list(zip(top.index, top.values))\n",
    "\n",
    "most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will print out the 10 LEAST common words said by each comedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_common_words = {}\n",
    "for comedian in copy_dtm_data.columns:\n",
    "    bottom = copy_dtm_data[comedian].sort_values(ascending=True).head(10)    \n",
    "    least_common_words[comedian] = list(zip(bottom.index, bottom.values))\n",
    "\n",
    "least_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a word cloud, representing the most common words\n",
    "### By plotting \"word clouds\", it's quite easy to see what words (in this case) is used the most. This kind of visualization can be very helpful in the NLP-area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "word_cloud = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=250, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [24, 8]\n",
    "\n",
    "comedian_names = ['Carlin', 'Gervais', 'Jefferies']\n",
    "\n",
    "# Create subplots for each comedian\n",
    "for index, comedian in enumerate(most_common_words):\n",
    "    word_cloud.generate(transcripts.transcript[comedian])\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(comedian_names[index])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's measure amount of profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By simply analyzing the most common words, we can see that at least 'fucking' and 'shit' is mentioned.<br>As we call the dtm_data - brackets - swearword, we can see the amount of times each comdeian has said that<br>perticular word. Here is an example with the word 'fucking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_data['fucking']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see multiple words, we can use Pandas function 'concat' and sort two similar words: 'Fuck' and 'fucking'<br> and add them together while also measure the amout of times the word 'shit' has been said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_profanity = pd.concat([dtm_data.fucking + dtm_data.fuck, dtm_data.shit], axis=1)\n",
    "data_profanity.columns = ['f_word', 's_word']\n",
    "data_profanity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "###  Main question I had was \"Who swears the most\"?<br>By only printing the swearword 'fucking', we can see that Jim Jefferies used that word way more than George Carlin and more than double the times than Ricky Gervais. So in summary, Jim Jefferies is the comedian who swears the most. At least with these selected transcripts, but it kind of reflects the comedians way of acting. Another 'happy' discovery is that the 'f-word' is said at least twice as much as the 's-word' by all comedians.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes this NLP introduction, we have:<br>- Made a setup<br>- Harvested data from URL-addresses<br>- Cleaned data<br>- Tokenized words and removed stop-words<br>- Created a DTM<br>- Displayed most and least said words from each comedian<br>- Measured amount of times a specific word has been said by each comedian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further implementations and tips:\n",
    "### Instead of seeing 'same words' in different forms, a tool called \"Lemmatization\" can help further to reform each word into it's root word. As an example: Call, called and calling all has comes from the root-word 'call'.<br><br>Cleaing data can allways be applied and it's fine to apply some cleaning techniques afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the end for part 2. Thank you for readning and following this introduction to NLP.<br>I hope it's of good use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
